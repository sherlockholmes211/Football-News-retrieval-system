{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qZ-TMc9SVHl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "####### After importing nltk, run the following only once ######\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "### pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlcodes (document):\n",
    "    \n",
    "    '''Removes HTML entity codes such as &amp from document and returns the clean document'''\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    \n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos (word):\n",
    "    \n",
    "    '''Returns the tag of usage of word depending on context'''\n",
    "    \n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop (str):\n",
    "    \n",
    "    '''Returns the lemmatized document after tokenization and stop word removal'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    \n",
    "    '''Returns true if text has no special characters, else returns false'''\n",
    "    \n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return TrueF\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words (text):\n",
    "    \n",
    "    '''Removes special characters from text and returns a clean string'''\n",
    "    \n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    \n",
    "    '''Cleans document_string by splitting very long strings and identifying garbage JSON and HTML and discarding'''\n",
    "    \n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "                    else:\n",
    "                        replace_with=' '.join(word for word in split)\n",
    "                        cleaned_doc = cleaned_doc.replace(word, replace_with)\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "count_dates = []\n",
    "\n",
    "def replace_dates(documentString, docID):\n",
    "    \n",
    "    '''Replaces dates of the format MM/DD and MM/DD/YYYY with DDmmmYYYY inside documentString'''\n",
    "    \n",
    "    regEx = '(([0-9]+(/)[0-9]+(/)[0-9]+)|([0-9]+(/)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    tmp = []\n",
    "    replace_with = []\n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp.append(date)\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            check_year = date[-3]\n",
    "            \n",
    "            if check_year == '/':\n",
    "                YY = date[-2:]\n",
    "                \n",
    "                if int(YY) <= 19:\n",
    "                    proper_date = date[:-2] + '20' + YY\n",
    "                    date = date.replace(date,proper_date)\n",
    "                else:\n",
    "                    proper_date = date[:-2] + '19' + YY\n",
    "                    date = date.replace(YY,('19'+YY))\n",
    "                    \n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        count_dates.append([docID, date])\n",
    "        newDate = newDate.replace(' ', '')\n",
    "        replace_with.append(newDate)\n",
    "        \n",
    "    for i in range(len(tmp)):\n",
    "        documentString = documentString.replace(tmp[i], replace_with[i])\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Reading persistent files\n",
    "\n",
    "import pickle\n",
    "import trie\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "data = np.load(\"datan.npy\", allow_pickle = True)\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = data[i][0]\n",
    "    get_index[data[i][0]] = i\n",
    "collection = None\n",
    "documentRoot = {}\n",
    "max_tf = {}\n",
    "\n",
    "with open('collection.pickle', 'rb') as handle:\n",
    "    collection = pickle.load(handle)\n",
    "with open('documentRoot.pickle', 'rb') as handle:\n",
    "    documentRoot = pickle.load(handle)\n",
    "with open('max_tf.pickle', 'rb') as handle:\n",
    "    max_tf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go9w6bB9rJfg",
    "outputId": "2e5b7bc8-2e68-4de8-f81d-a142a989c902"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['messi', 'india']\n"
     ]
    }
   ],
   "source": [
    "# Processing query\n",
    "\n",
    "import unidecode\n",
    "\n",
    "query = \"messi india\"\n",
    "final_query = replace_dates(query, -1)\n",
    "final_query = lemma_stop(final_query)\n",
    "\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "\n",
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfT-kdUqrJfp",
    "outputId": "3c95efd0-76c9-4dfe-e425-e79c089931ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Term in query =  messi\n",
      "\n",
      "df =  607\n",
      "idf =  1.61475131759678\n",
      "-------------------------------------\n",
      "Term in query =  india\n",
      "\n",
      "df =  757\n",
      "idf =  1.5188441291719648\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607522255-17750\n",
      "Keywords:\n",
      "\n",
      "WATCH: Sunil Chhetri scores two brilliant goals to hand India Intercontinental Cup title\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "india -1.5188441291719648 1\n",
      "\n",
      "\n",
      "Sunil Chhetri equalled Lionel Messi's record of 64 International goals as India beat Kenya 2-0 to lift Intercontinental Cup on Sunday.  ... \n",
      "\n",
      "tf-idf score= 9.81942042400981\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607522250-17708\n",
      "Keywords:\n",
      "\n",
      "Unfair to compare me with ‘world stars’ Lionel Messi, Cristiano Ronaldo, says Sunil Chhetri\n",
      "\n",
      "title score =  0\n",
      "messi -1.211063488197585 1\n",
      "india -1.1391330968789735 1\n",
      "\n",
      "\n",
      "Indian skipper Sunil Chhetri has scored 62 goals for India and need three more to surpass Lionel Messi in the list of all-time leading international goal scorers among currently active players.  ... \n",
      "\n",
      "tf-idf score= 7.364565318007356\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607523234-22225\n",
      "Keywords:\n",
      "\n",
      "The Best FIFA Football Awards 2019 Live Streaming: When and where to watch, IST, nominations, all you need to know\n",
      "\n",
      "title score =  0\n",
      "messi -1.0765008783978534 1\n",
      "india -1.0125627527813097 1\n",
      "\n",
      "\n",
      "The Best FIFA Football Awards 2019 Live Streaming, India Date and Time: Cristiano Ronaldo and Lionel Messi both have won The Best FIFA Men's Player award five times each, with Virgil van Dijk competing for the first time.  ... \n",
      "\n",
      "tf-idf score= 6.546280282673206\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607518106-2058\n",
      "Keywords:\n",
      "\n",
      "Luis Suarez, Neymar keep Barcelona on track, Real Madrid beat Las Palmas\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Despite the absence of Lionel Messi, Neymar and Luis Suarez led Barcelona to 2-0 win.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607518475-3525\n",
      "Keywords:\n",
      "\n",
      "Lionel Messi fires hat-trick on return as Argentina hammer Panama 5-0, enter Copa America quarters\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Lionel Messi came on after 61 minutes and took only 18 to fire a hat-trick in Argentina's 5-0 win over Panama.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607523394-23010\n",
      "Keywords:\n",
      "\n",
      "Messi says Ronaldo duel will last forever\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Lionel Messi said that El Clasicos used to be 'more special' when Ronaldo was there in the Spanish capital.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607518850-4865\n",
      "Keywords:\n",
      "\n",
      "Lionel Messi allays injury concerns, says ‘everything is fine’\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Lionel Messi has played down the seriousness of a hamstring injury as he spurs Argentina to a 1-0 win over Uruguay.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607522152-17408\n",
      "Keywords:\n",
      "\n",
      "Neymar to Real Madrid will be major blow to Barcelona, says Lionel Messi\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Lionel Messi said he had spoken to Neymar about rumours linking him with a return to Spain for La Liga rivals Real Madrid.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607519522-7512\n",
      "Keywords:\n",
      "\n",
      "Cristiano Ronaldo favoured to win his 4th FIFA award as world’s best\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Cristiano Ronaldo is favoured to beat Lionel Messi and Antoine Griezmann when winners of the Best FIFA Football Awards are presented in Zurich.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1607523711-24560\n",
      "Keywords:\n",
      "\n",
      "Lionel Messi would be ‘welcome’ at PSG: Thomas Tuchel\n",
      "\n",
      "title score =  0\n",
      "messi -1.61475131759678 1\n",
      "\n",
      "\n",
      "Paris St Germain manager Thomas Tuchel says Lionel Messi would be welcome at the club if he ever decided to leave Barcelona but he does not expect the Argentine forward to do so.  ... \n",
      "\n",
      "tf-idf score= 2.607421817680537\n",
      "\n",
      "\n",
      "============================================\n",
      "April 24 2017 8\n",
      "June 28 2016 8\n",
      "June 5 2018 8\n",
      "January 7 2019 8\n",
      "June 7 2018 7\n",
      "June 13 2017 6\n",
      "June 11 2018 6\n",
      "September 5 2019 6\n",
      "January 11 2019 6\n",
      "July 3 2019 5\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    \n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        \n",
    "        tf_doc = documentRoot[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        tfidf_doc = (tf_doc)\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "            \n",
    "# print(title_score)\n",
    "\n",
    "for docID in scores:\n",
    "    \n",
    "    #if documentLength[docID] != 0:\n",
    "    scores[docID] *= factor[docID]\n",
    "    if docID in title_score:\n",
    "        scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    \n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(data[get_index[sorted_scores[i][0]]][2])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(documentRoot[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    \n",
    "    for word in data[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word, -1)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')\n",
    "#print(sorted_scores)\n",
    "dates = []\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    split_l = re.split(\",|\\s\",data[get_index[sorted_scores[i][0]]][5])\n",
    "    s = split_l[0] +\" \"+ split_l[1] + \" \" + split_l[3]\n",
    "    #print(s)\n",
    "    dates.append(s)\n",
    "    #print(dates)\n",
    "    \n",
    "#dates.sort(key = lambda date: datetime.strptime(date, '%B %d %Y')) \n",
    "#print(dates)\n",
    "count1 = 0\n",
    "for keys in Counter(dates).most_common():\n",
    "    print(keys[0],keys[1])\n",
    "    count1 += 1\n",
    "    if count1 == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['messi']\n"
     ]
    }
   ],
   "source": [
    "# Processing query\n",
    "\n",
    "import unidecode\n",
    "\n",
    "query = \"messi\"\n",
    "final_query = replace_dates(query, -1)\n",
    "final_query = lemma_stop(final_query)\n",
    "\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "\n",
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-35-4bd5f46bf503>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-4bd5f46bf503>\"\u001b[1;36m, line \u001b[1;32m90\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import queue\n",
    "\n",
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    " #   print('-------------------------------------')\n",
    " #   print('Term in query = ', query_term)\n",
    "#    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    \n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "#    print('df = ',df)\n",
    "#    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        \n",
    "        tf_doc = documentRoot[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        tfidf_doc = (tf_doc)\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "            \n",
    "# print(title_score)\n",
    "\n",
    "for docID in scores:\n",
    "    \n",
    "    #if documentLength[docID] != 0:\n",
    "    scores[docID] *= factor[docID]\n",
    "    if docID in title_score:\n",
    "        scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "#print('\\n\\n')\n",
    "#print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    \n",
    "   # print()\n",
    "    docID = sorted_scores[i][0]\n",
    "   # print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "   # print('Keywords:')\n",
    "   # print()\n",
    "   # print(data[get_index[sorted_scores[i][0]]][2])\n",
    "   # print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "  #      print('title score = ',0)\n",
    "    else:\n",
    "        pass\n",
    " #       print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "#        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "#        print(documentRoot[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "#     print()\n",
    "#     print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    \n",
    "    for word in data[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word, -1)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    #if not at_start:\n",
    "        #print('...', end = ' ')\n",
    "    #while words_before.qsize() > 0:\n",
    "        #print(words_before.get(), end = ' ')\n",
    "    #print(display, end = ' ')\n",
    "    #print('...', end = ' ')\n",
    "    #print('\\n')\n",
    "    #print('tf-idf score=', sorted_scores[i][1])\n",
    "    #print('\\n')\n",
    "    #print('============================================')\n",
    "#print(sorted_scores)\n",
    "dates = []\n",
    "from collections import Counter\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    split_l = re.split(\",|\\s\",data[get_index[sorted_scores[i][0]]][5])\n",
    "    s = split_l[0] +\" \"+ split_l[1] + \" \" + split_l[3]\n",
    "    #print(s)\n",
    "    dates.append(s)\n",
    "    #print(dates)\n",
    "    \n",
    "#dates.sort(key = lambda date: datetime.strptime(date, '%B %d %Y')) \n",
    "#print(dates)\n",
    "count1 = 0\n",
    "dic1 = {}\n",
    "for keys in Counter(dates).most_common():\n",
    "    #print(keys[0],keys[1])\n",
    "    dic1[keys[0]] = keys[1]\n",
    "    count1 += 1\n",
    "    if count1 == 5:\n",
    "        break\n",
    "\n",
    "plt.bar(*zip(*dic1.items()))\n",
    "\n",
    "plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
