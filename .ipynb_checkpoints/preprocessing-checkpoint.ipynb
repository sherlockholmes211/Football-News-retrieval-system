{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__1Nxb4gRIeN"
   },
   "source": [
    "Done so far :\n",
    "\n",
    "\n",
    "*   Lemmatization\n",
    "*   Stop Words Removal\n",
    "\n",
    "Verify :\n",
    "\n",
    "* Normalization - removing accents, etc.\n",
    "* Dates replaced with strings\n",
    "* Case-folding\n",
    "* Removed HTML entity codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhRPXd06Hkpo",
    "outputId": "57b3984e-bc49-4f3b-a3d3-d29ca48caa29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordninja in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "pip install wordninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qZ-TMc9SVHl",
    "outputId": "00fe7e36-6884-4f96-cd08-9a31c65e3a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import wordninja \n",
    " \n",
    "####### After importing nltk, run the following only once ######\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "###pip install wordninja ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn-b0w8-hmKn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3H4pv21qhRaW"
   },
   "outputs": [],
   "source": [
    "def remove_htmlcodes(document):\n",
    "    \n",
    "    replacement = {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                  }\n",
    "    for str in replacement:\n",
    "        document = document.replace(str, replacement[str])\n",
    "        \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpQkEVQOSVHr"
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    tag=nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict={\"J\": wordnet.ADJ, \n",
    "              \"N\": wordnet.NOUN,\n",
    "              \"V\": wordnet.VERB,\n",
    "              \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag,wordnet.NOUN)\n",
    "\n",
    "def lemma_stop(str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$]\\d\\[+|\\S+,-')\n",
    "    tokenized = tokenizer.tokenize(str)\n",
    "    lemmatized = [lemmatizer.lemmatize(w,get_wordnet_pos(w)) for w in tokenized]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in lemmatized if w.lower() not in stop_words]\n",
    "    after_lemma_stop = ' '.join(w for w in filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PvIwQ6ySVHv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data.npy\n",
    "# data.npy is a 2D array containing the dataset information as\n",
    "# data[i][0] : docID of ith document\n",
    "# data[i][1] : title of ith document\n",
    "# data[i][4] : content of ith document\n",
    "data = pd.read_csv(\"indianexpress.csv\")\n",
    "data = np.array(data)\n",
    "np.save(\"datan\",data)\n",
    "data = np.load(\"datan.npy\",allow_pickle = True) \n",
    "#data = np.load('data.npy',allow_pickle = True)\n",
    "# sentence = data[0][4]\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y6aQ7ZcPCH4d",
    "outputId": "6d821e4d-31d2-4b2f-f556-ea116a8ec159",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "# creating a map {index_in_data_npy, docID}\n",
    "\n",
    "# ex. if ith element in data has docID j,\n",
    "# get_docID[i] will return j\n",
    "\n",
    "get_docID = {}\n",
    "get_index = {}\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "for i in range(0, len(data)) :\n",
    "    get_docID[i] = data[i][0]\n",
    "    get_index[data[i][0]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "790C4ZoWhRaX"
   },
   "outputs": [],
   "source": [
    "def is_not_credible (text):\n",
    "    match = re.search(r'[!@#?&{}()]', text)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZeBOk8PhRaX"
   },
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    text = re.sub('[!@#?&{}()]', '', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7F]',\" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wnQZK3EhRaX"
   },
   "outputs": [],
   "source": [
    "def clean_document (document_string):\n",
    "    cleaned_doc = document_string\n",
    "    for word in document_string.split():\n",
    "                if is_not_credible(word):\n",
    "                    temp= scrub_words(word)\n",
    "                    split=wordninja.split(temp)\n",
    "                    if len(split)>7:\n",
    "                          cleaned_doc = cleaned_doc.replace(word,'')\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1_nZ_FvhRaX"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def replace_dates(documentString):\n",
    "    \n",
    "    regEx = '(([0-9]+(/|\\\\.|-)[0-9]+(/|\\\\.|-)[0-9]+)|([0-9]+(/|\\\\.|-)[0-9]+))'\n",
    "    iterator = re.finditer(regEx, documentString)\n",
    "    listOfDates = [(m.start(0), m.end(0)) for m in iterator]\n",
    "    \n",
    "    for indices in listOfDates:\n",
    "        date = documentString[indices[0]:indices[1]]\n",
    "        tmp = date\n",
    "        date = date.replace('.', '/')\n",
    "        date = date.replace('-', '/')\n",
    "        count = date.count('/')\n",
    "        newDate = ''\n",
    "        if count == 2:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d/%Y').strftime('%d %b %Y')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "        else:\n",
    "            try:\n",
    "                newDate = datetime.strptime(date, '%m/%d').strftime('%d %b')\n",
    "            except ValueError as ve:\n",
    "                newDate = date\n",
    "                \n",
    "        newDate = newDate.replace(' ', '')\n",
    "        documentString = documentString.replace(tmp, newDate)\n",
    "        # print(newDate)\n",
    "    \n",
    "    return documentString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH7buH_WhRaX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XMoBuuDhRaX"
   },
   "outputs": [],
   "source": [
    "# Before cleaning \n",
    "\n",
    "\n",
    "#print(len(lemma_stop(data[get_index['1605443934-507']][4])))\n",
    "#print(lemma_stop(data[get_index['1605443934-507']][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-evZ9TfhRaX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After cleaning (removing JSON, HTML, etc)\n",
    "\n",
    "#cleaned_doc=clean_document(data[get_index['1605443934-507']][4])\n",
    "#print(len(lemma_stop(cleaned_doc)))\n",
    "#print(lemma_stop(cleaned_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbw8tNbrJfL"
   },
   "source": [
    "*Run the following cell once after all pre-processing (removing JSON etc), and store final lemmatized contents of all docs:* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibq9gAauMI5Q"
   },
   "outputs": [],
   "source": [
    "# Run once after all pre-processing \n",
    "\n",
    "\n",
    "# # Tester code\n",
    "# import time\n",
    "\n",
    "# s = time.time()\n",
    "\n",
    "# for i in range(0, len(data)) :\n",
    "#     f_content = clean_document(data[i][4])\n",
    "#     contents = f_content\n",
    "#     # print(contents)\n",
    "#     final = lemma_stop (contents)\n",
    "# #     print(type(final))\n",
    "#     # print (final)\n",
    "    \n",
    "# print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JokFmLV8hRaX"
   },
   "outputs": [],
   "source": [
    "# creating a temporary smaller dataset\n",
    "\n",
    "subset = []\n",
    "counter = 0\n",
    "for document in data:\n",
    "    subset.append(document)\n",
    "    counter += 1\n",
    "    if counter == 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRqA8lwehRaX",
    "outputId": "11ac5ec9-384f-49ba-db1e-bd1eb4e9ec2e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:09<00:00, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.17977499961853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "contents = []\n",
    "for document in tqdm(subset):\n",
    "    # actually modifying the document\n",
    "    document[4] = remove_htmlcodes(document[4])\n",
    "    \n",
    "    # not actually modifying the document\n",
    "    modifiedContent = replace_dates(document[4])\n",
    "    modifiedContent = lemma_stop(clean_document(modifiedContent))\n",
    "    modifiedTitle = lemma_stop(clean_document(document[2]))\n",
    "    \n",
    "    # case-folding\n",
    "    for i in range(len(modifiedContent)):\n",
    "        modifiedContent[i] = modifiedContent[i].lower()\n",
    "    \n",
    "    # modifiedTitle = lemma_stop((document[1]))\n",
    "    titles.append(modifiedTitle)\n",
    "    contents.append(modifiedContent)\n",
    "    \n",
    "print(time.time() - start)  # 110.26236414909363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSWPALO7jzOb",
    "outputId": "225faea6-b365-4a25-a5ee-07122fd5b2a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5l8HSfuohRaX"
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "contents_temp = contents\n",
    "\n",
    "titles_temp = titles\n",
    "\n",
    "for i in range(148):\n",
    "    for j in range(len(contents[i])):\n",
    "        contents[i][j] = unidecode.unidecode(contents[i][j])\n",
    "    for j in range(len(titles[i])):\n",
    "        titles[i][j] = unidecode.unidecode(titles[i][j])\n",
    "# for document in contents_temp:\n",
    "#     for word in document:\n",
    "#         word = unidecode.unidecode(word)\n",
    "# for title in titles_temp:\n",
    "#     for word in title:\n",
    "#         word = unidecode.unidecode(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr-HGZhDrJfT"
   },
   "outputs": [],
   "source": [
    "import trie\n",
    "\n",
    "# Create map from docID of the document to an object of class Node \n",
    "# (i.e, the corresponding document trie structure)\n",
    "# ex. if the docID of the document is 1, \n",
    "# getReference[1] gives the object which is the trie structure of docID 1\n",
    "\n",
    "getReference = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M63H5wohRaX"
   },
   "outputs": [],
   "source": [
    "documentRoot = []\n",
    "collection = trie.CollectionNode()\n",
    "\n",
    "# initializing the root for 1000 documents\n",
    "for i in range(148):\n",
    "    newDocument = trie.Node()\n",
    "    documentRoot.append(newDocument)\n",
    "    getReference[get_docID[i]] = newDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9cGiLfuhRaY",
    "outputId": "639395aa-9502-491e-85dd-e6e6c60dbc7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:00<00:00, 203.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7356045246124268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# creating the documents\n",
    "\n",
    "max_tf = {}\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "for i in tqdm(range(148)):\n",
    "    for w in contents_temp[i]:\n",
    "        collection.add_document(w, 0, get_docID[i])\n",
    "        documentRoot[i].add(w, 0)\n",
    "        if get_docID[i] in max_tf:\n",
    "            max_tf[get_docID[i]] = max(documentRoot[i].count_words(w, 0), max_tf[get_docID[i]])\n",
    "        else:\n",
    "            max_tf[get_docID[i]] = documentRoot[i].count_words(w, 0)\n",
    "    for w in titles_temp[i]:\n",
    "        collection.add_title(w, 0, get_docID[i])\n",
    "        \n",
    "print(time.time() - start)  #39.19705152511597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eB9AxokfhRaY",
    "outputId": "de59d2a7-8c74-449b-cbbb-8cd144b28630"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/148 [00:00<00:07, 19.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:05<00:00, 24.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import queue\n",
    "\n",
    "documentLength = {}\n",
    "N = len(documentRoot)\n",
    "print(\"hello\")\n",
    "print(N)\n",
    "for i in tqdm(range(len(documentRoot))):\n",
    "    \n",
    "    docID = get_docID[i]\n",
    "    length = 0\n",
    "    document = documentRoot[i]\n",
    "    q = queue.Queue()\n",
    "    q.put([document, ''])\n",
    "    \n",
    "    while q.qsize() > 0:\n",
    "\n",
    "        current = q.get()\n",
    "        reference = current[0]\n",
    "        word = current[1]\n",
    "\n",
    "        if reference.words > 0:\n",
    "            df = len(collection.get_doc_list(word, 0))\n",
    "            idf = math.log10(N/df)\n",
    "            # print(word, reference.words, df)\n",
    "            length += (reference.words * idf) ** 2\n",
    "\n",
    "        for i in range(256):\n",
    "            if reference.children.get(i) is not None:\n",
    "                new_word = word + chr(i)\n",
    "                q.put([reference.children[i], new_word])\n",
    "\n",
    "    # print(length**0.5)\n",
    "    documentLength[docID] = length**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auUhJ1AyhRaY",
    "outputId": "e8bb2c75-3e7d-4ded-9055-df3b9f484c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1248\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(documentRoot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go9w6bB9rJfg",
    "outputId": "ce75612b-2151-4719-c11f-709572092d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon', 'us']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "query = 'Amazon US'\n",
    "final_query = replace_dates(query)\n",
    "final_query = lemma_stop(final_query)\n",
    "for i in range(len(final_query)):\n",
    "    final_query[i] = unidecode.unidecode(final_query[i])\n",
    "    \n",
    "    # case-folding\n",
    "    final_query[i] = final_query[i].lower()\n",
    "print(final_query)\n",
    "print(len(final_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Kqgk9hrJfj"
   },
   "outputs": [],
   "source": [
    "tf_query = {}\n",
    "for w in final_query:\n",
    "    if w not in tf_query:\n",
    "        tf_query[w] = 1\n",
    "    else:\n",
    "        tf_query[w] += 1\n",
    "        \n",
    "    # Test code just to see distribution of query terms in the documents\n",
    "    \n",
    "    # print(w)\n",
    "    # df = len(collection.get_doc_list(w,0))\n",
    "    # print(collection.get_doc_list(w,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVFuxoqchRaY"
   },
   "outputs": [],
   "source": [
    "# print(type(documentRoot[1]), get_docID[1])\n",
    "# document = documentRoot[1]\n",
    "# N = len(documentRoot)\n",
    "\n",
    "# import queue\n",
    "# import math\n",
    "\n",
    "# length = 0\n",
    "# q = queue.Queue()\n",
    "# q.put([document, ''])\n",
    "\n",
    "# while q.qsize() > 0:\n",
    "    \n",
    "#     current = q.get()\n",
    "#     reference = current[0]\n",
    "#     word = current[1]\n",
    "    \n",
    "#     if reference.words > 0:\n",
    "#         df = len(collection.get_doc_list(word, 0))\n",
    "#         idf = math.log10(N/df)\n",
    "#         # print(word, reference.words, df)\n",
    "#         length += (reference.words * idf) ** 2\n",
    "    \n",
    "#     for i in range(256):\n",
    "#         if reference.children[i] is not None:\n",
    "#             new_word = word + chr(i)\n",
    "#             q.put([reference.children[i], new_word])\n",
    "\n",
    "# print(length**0.5)\n",
    "# print(replace_dates(subset[get_index[104]][4]))\n",
    "# replace_dates('12/12')\n",
    "# lemma_stop('12Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBHDg-lJrJfo"
   },
   "source": [
    "***Ranked Retrieval based on TF-IDF Score :***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfT-kdUqrJfp",
    "outputId": "956f82e5-e660-4a44-8820-162fe24d5254",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1605443873-487', '1605443746-452', '1605443932-506', '1605443404-403', '1605443362-397', '1605443642-423', '1605443913-500', '1605443819-472', '1605443812-470', '1605443669-430', '1605443891-493', '1605443616-416', '1605443803-468', '1605443639-422', '1605443784-463', '1605443319-386', '1605443809-469', '1605443568-413', '1605443739-450', '1605443763-458', '1605443336-391', '1605443714-443', '1605443878-489', '1605443934-507', '1605443928-505', '1605443678-433', '1605443590-415', '1605443357-396', '1605443705-440', '1605443646-424', '1605443343-393', '1605443833-476', '1605443438-405', '1605443269-372', '1605443383-401', '1605443331-390', '1605443708-441', '1605443862-484', '1605443748-453', '1605443699-439', '1605443777-461', '1605443619-417', '1605443287-377', '1605443829-475', '1605443312-384', '1605443899-496', '1605443423-404', '1605443325-388', '1605443449-406', '1605443905-498', '1605443793-465', '1605443622-418', '1605443865-485', '1605443925-504', '1605443826-474', '1605443796-466', '1605443260-370', '1605443837-477', '1605443386-402', '1605443916-501', '1605443464-407', '1605443537-411', '1605443799-467', '1605443652-426', '1605443369-399', '1605443684-435', '1605443919-502', '1605443322-387'}\n",
      "-------------------------------------\n",
      "Term in query =  amazon\n",
      "\n",
      "set()\n",
      "df =  68\n",
      "idf =  0.33775280268872104\n",
      "{'1605443868-486', '1605443339-392', '1605443719-444', '1605443913-500', '1605443751-454', '1605443812-470', '1605443581-414', '1605443891-493', '1605443281-375', '1605443616-416', '1605443285-376', '1605443894-494', '1605443784-463', '1605443319-386', '1605443754-455', '1605443714-443', '1605443336-391', '1605443934-507', '1605443857-482', '1605443908-499', '1605443590-415', '1605443475-408', '1605443635-421', '1605443897-495', '1605443833-476', '1605443438-405', '1605443735-449', '1605443788-464', '1605443255-368', '1605443257-369', '1605443287-377', '1605443619-417', '1605443899-496', '1605443325-388', '1605443729-447', '1605443712-442', '1605443655-427', '1605443780-462', '1605443291-378', '1605443845-479', '1605443464-407', '1605443885-491', '1605443851-480', '1605443799-467', '1605443244-365', '1605443293-379', '1605443854-481', '1605443722-445', '1605443322-387', '1605443742-451'}\n",
      "-------------------------------------\n",
      "Term in query =  us\n",
      "\n",
      "set()\n",
      "df =  50\n",
      "idf =  0.4712917110589386\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443934-507\n",
      "Keywords:\n",
      "\n",
      "Amazon US Employees Press for Election Day Holiday\n",
      "\n",
      "title score =  0\n",
      "amazon -0.33775280268872104 9\n",
      "us -0.31419447403929235 3\n",
      "\n",
      "\n",
      "More than 3,000 Amazon employees have signed a petition asking the technology and retail giant to provide a holiday for voting in the November 3 US election, organisers said.The petition was organised by Amazon Employees for Climate Justice, a group active on several social issues involving the company.\"Removing barriers to voting is critical  ... \n",
      "\n",
      "tf-idf score= 0.5274543674895266\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443899-496\n",
      "Keywords:\n",
      "\n",
      "Nokia Gives Google Nod for IT Infrastructure\n",
      "\n",
      "title score =  0\n",
      "us -0.3770333688471509 3\n",
      "amazon -0.20265168161323263 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.46899100977650854\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443322-387\n",
      "Keywords:\n",
      "\n",
      "TikTok Rival Triller Said to Explore Deal to Go Public\n",
      "\n",
      "title score =  0\n",
      "us -0.38713247694127095 9\n",
      "amazon -0.18093900144038627 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.459595113001202\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443325-388\n",
      "Keywords:\n",
      "\n",
      "Huawei Can Continue Serving European 5G Clients Despite US Sanctions, Senior Executive Says\n",
      "\n",
      "title score =  0\n",
      "us -0.35346878329420395 6\n",
      "amazon -0.1829494347897239 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.43398621641509744\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443619-417\n",
      "Keywords:\n",
      "\n",
      "Microsoft Targets Malware Vendor Trickbot Amid US Election Fears\n",
      "\n",
      "title score =  0\n",
      "us -0.33663693647067044 3\n",
      "amazon -0.19300160153641202 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.4285011534439614\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443464-407\n",
      "Keywords:\n",
      "\n",
      "OECD Says Global Deal on Taxing Tech Giants Won’t Be Secured This Year\n",
      "\n",
      "title score =  0\n",
      "us -0.32627887688695745 5\n",
      "amazon -0.1948573861665698 2\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.42162243453841336\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443336-391\n",
      "Keywords:\n",
      "\n",
      "Five Eyes Alliance, India, Japan Demand ‘Backdoors’ to Access Encrypted Apps\n",
      "\n",
      "title score =  0\n",
      "us -0.2945573194118366 2\n",
      "amazon -0.18998595151240558 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.39201707501460403\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443438-405\n",
      "Keywords:\n",
      "\n",
      "iPhone 12 to Offer 5G Speeds US Networks Can’t Deliver: Analysis\n",
      "\n",
      "title score =  0\n",
      "us -0.30297324282360333 4\n",
      "amazon -0.18093900144038627 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.39150654635709814\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443287-377\n",
      "Keywords:\n",
      "\n",
      "Microsoft to Let Some Employees Work From Home Permanently\n",
      "\n",
      "title score =  0\n",
      "us -0.30297324282360333 4\n",
      "amazon -0.18093900144038627 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.39150654635709814\n",
      "\n",
      "\n",
      "============================================\n",
      "\n",
      "doc ID =  1605443812-470\n",
      "Keywords:\n",
      "\n",
      "Canon PowerShot Zoom Pocket-Sized Monocular Telephoto Camera With 400mm Optical Zoom Launched\n",
      "\n",
      "title score =  0\n",
      "us -0.2945573194118366 3\n",
      "amazon -0.1829494347897239 1\n",
      "\n",
      "\n",
      "... can subscribe to via Apple Podcasts, Google Podcasts, or RSS, download the episode, or just hit the play button below.  ... \n",
      "\n",
      "tf-idf score= 0.38632421976422476\n",
      "\n",
      "\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# scores[i] stores the dot product of the tf-idf score vectors of the query and document of docID i in the corpus\n",
    "scores = {}\n",
    "title_score = {}\n",
    "\n",
    "# N is the total number of documents in the corpus\n",
    "N = len(documentRoot)\n",
    "\n",
    "# wordsInDoc[i] is a sorted list of (word, score) tuples where\n",
    "# score is the tf-idf score for the (word, <ith doc>) pair\n",
    "wordsInDoc = {}\n",
    "\n",
    "factor = {}\n",
    "\n",
    "import math\n",
    "import bisect\n",
    "\n",
    "for query_term in tf_query:\n",
    "    \n",
    "    docs_having_query_term = collection.get_doc_list(query_term, 0)\n",
    "    print(docs_having_query_term)\n",
    "    df = len(docs_having_query_term)\n",
    "    idf = 0\n",
    "    \n",
    "    print('-------------------------------------')\n",
    "    print('Term in query = ', query_term)\n",
    "    # print('List of documents with this term=', docs_having_query_term)\n",
    "    print()\n",
    "    \n",
    "    if df == 0:\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = math.log10(N/df)\n",
    "        \n",
    "    docs_having_query_term_in_title = collection.get_title_list(query_term,0)\n",
    "    print(docs_having_query_term_in_title)\n",
    "    for docID in docs_having_query_term_in_title:\n",
    "        if docID in title_score:\n",
    "            title_score[docID] += idf\n",
    "        else:\n",
    "            title_score[docID] = idf\n",
    "        \n",
    "    print('df = ',df)\n",
    "    print('idf = ',idf)\n",
    "    \n",
    "    tfidf_query = tf_query[query_term] * idf\n",
    "        \n",
    "    for docID in docs_having_query_term:\n",
    "        # print(docID)\n",
    "        tf_doc = getReference[docID].count_words(query_term, 0)\n",
    "        tf_doc = 0.5 + 0.5*tf_doc/max_tf[docID]\n",
    "        # print('tf for doc',docID,'is',tf_doc)\n",
    "        # tfidf_doc_query = tf_doc * idf\n",
    "        # tfidf_doc = 1 + math.log10(tf_doc)\n",
    "        tfidf_doc = (tf_doc)\n",
    "        # tfidf_doc_query = (tf_doc)\n",
    "        \n",
    "        # print('tfidf for doc',doc,'is',tfidf_doc)\n",
    "        # print()\n",
    "        \n",
    "        if docID not in scores:\n",
    "            scores[docID] = (tfidf_query * tfidf_doc)\n",
    "            wordsInDoc[docID] = []\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] = idf\n",
    "        else:\n",
    "            scores[docID] += (tfidf_query * tfidf_doc)\n",
    "            bisect.insort(wordsInDoc[docID], [-tfidf_query * tfidf_doc, query_term])\n",
    "            factor[docID] += idf\n",
    "print(title_score)\n",
    "for docID in scores:\n",
    "    if documentLength[docID] != 0:\n",
    "#         scores[docID] = scores[docID]/ math.sqrt(documentLength[docID])\n",
    "        scores[docID] *= factor[docID]\n",
    "        if docID in title_score:\n",
    "            scores[docID] *= 1 + title_score[docID]\n",
    "\n",
    "\n",
    "sorted_scores = sorted(scores.items(), key = lambda kv : kv[1] , reverse = True)\n",
    "\n",
    "maxshow = min(10, len(scores))\n",
    "\n",
    "print('\\n\\n')\n",
    "print('============================================')\n",
    "\n",
    "for i in range(maxshow):\n",
    "    # print(i)\n",
    "    print()\n",
    "    docID = sorted_scores[i][0]\n",
    "    print('doc ID = ', docID)\n",
    "    cnt = 0\n",
    "    print('Keywords:')\n",
    "    print()\n",
    "    print(subset[get_index[sorted_scores[i][0]]][2])\n",
    "    print()\n",
    "    if sorted_scores[i][0] not in title_score:\n",
    "        print('title score = ',0)\n",
    "    else:\n",
    "        print('title score = ',title_score[sorted_scores[i][0]])\n",
    "    for j in range(len(wordsInDoc[docID])):\n",
    "        print(wordsInDoc[docID][j][1], wordsInDoc[docID][j][0], end = ' ')\n",
    "        print(getReference[docID].count_words(wordsInDoc[docID][j][1], 0))\n",
    "    print()\n",
    "    print()\n",
    "    count = 0\n",
    "    found = 0\n",
    "    words_before=queue.Queue()\n",
    "    at_start = 1\n",
    "    display = \"\"\n",
    "    for word in subset[get_index[docID]][4].split():\n",
    "            \n",
    "        check_with=replace_dates(word)\n",
    "        check_with = check_with.lower()\n",
    "        if len(lemma_stop(check_with)) > 0:\n",
    "            check_with=lemma_stop(check_with)[0]\n",
    "        else:\n",
    "            check_with=word\n",
    "        \n",
    "        if check_with == wordsInDoc[docID][0][1]:\n",
    "            found=1\n",
    "            \n",
    "        if found == 1:\n",
    "            display = display + word + \" \"\n",
    "            count += 1\n",
    "            if count == 50:\n",
    "                break\n",
    "        if found == 0:\n",
    "            words_before.put(word)\n",
    "            if words_before.qsize()>20:\n",
    "                remove=words_before.get()\n",
    "                at_start=0\n",
    "                \n",
    "    if not at_start:\n",
    "        print('...', end = ' ')\n",
    "    while words_before.qsize() > 0:\n",
    "        print(words_before.get(), end = ' ')\n",
    "    print(display, end = ' ')\n",
    "    print('...', end = ' ')\n",
    "    print('\\n')\n",
    "    print('tf-idf score=', sorted_scores[i][1])\n",
    "    print('\\n')\n",
    "    print('============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqsqWNnFNOhO"
   },
   "source": [
    "***Original Text :***\n",
    "\n",
    "      We’re back in Dale Cooper’s position, wandering through a freshly revived world, and trying to catch up with the ways it’s moved on in his absence.\n",
    "\n",
    "***Processed Text :***\n",
    "\n",
    "      We back Dale Cooper position wander freshly revive world try catch way move absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhZBQswuhRaY"
   },
   "outputs": [],
   "source": [
    "#print(' '.join(contents[get_index[51]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB1Qyux1hRaY"
   },
   "outputs": [],
   "source": [
    "# import wordninja\n",
    "# print(wordninja.split('DragonSlayers'))\n",
    "# str = '&nbsp;&amp;&quot;&copy;&reg;&trade;&ldquo;&rdquo;&lsquo;&rsquo;&laquo;&raquo;&lsaquo;&rsaquo;&sect;&para;&bull;&middot;&hellip;&brvbar;&ndash;&mdash;'\n",
    "# str = str.replace(';', ' ')\n",
    "# str = str.replace('&', '&amp')\n",
    "# print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_c7wi3uhRaY",
    "outputId": "9cc01fca-bb39-4ba9-ae85-da73dd785f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing else matters.\"\n"
     ]
    }
   ],
   "source": [
    "test = 'nothing else matters.&amprdquo'\n",
    "# print(test)\n",
    "# print(test.replace('&ampnbsp',' '))\n",
    "\n",
    "replacement =   {\n",
    "                    \"&ampnbsp\": ' ',\n",
    "                    \"&ampamp\": '&',\n",
    "                    \"&ampquot\": '\\'',\n",
    "                    \"&ampldquo\": '\\\"',\n",
    "                    \"&amprdquo\": '\\\"',\n",
    "                    \"&amplsquo\": '\\'',\n",
    "                    \"&amprsquo\": '\\'',\n",
    "                    \"&amphellip\": '...',\n",
    "                    \"&ampndash\": '-',\n",
    "                    \"&ampmdash\": '-'\n",
    "                }\n",
    "for str in replacement:\n",
    "    test = test.replace(str, replacement[str])\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1i_DDqQqhRaY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
